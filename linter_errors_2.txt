PLR0904 Too many public methods (29 > 20)
    --> health_monitor.py:131:1
     |
 131 | / class SessionHealthMonitor:
 132 | |     """
 133 | |     Comprehensive session health monitoring with predictive analytics.
 134 | |     """
 135 | |
 136 | |     _action6_callback_registered: bool
 137 | |
 138 | |     def __init__(self) -> None:
 139 | |         self.metrics_history: dict[str, deque[tuple[float, float]]] = {}
 140 | |         self.current_metrics: dict[str, HealthMetric] = {}
 141 | |         self.alerts: list[HealthAlert] = []
 142 | |         self.health_score_history: deque[tuple[float, float]] = deque(maxlen=100)
 143 | |         self.session_start_time = time.time()
 144 | |         self.last_health_check = time.time()
 145 | |         self.monitoring_active = False
 146 | |         self.lock = threading.Lock()
 147 | |
 148 | |         # Performance tracking
 149 | |         self.api_response_times: deque[float] = deque(maxlen=50)
 150 | |         self.error_counts: dict[str, int] = {}
 151 | |         self.page_processing_times: deque[float] = deque(maxlen=20)
 152 | |         self.memory_usage_history: deque[float] = deque(maxlen=30)
 153 | |
 154 | |         # Enhanced error rate monitoring - PERFORMANCE OPTIMIZED
 155 | |         self.error_timestamps: deque[float] = deque(maxlen=2000)  # Increased for 20+ hour sessions
 156 | |         self.error_rate_warnings_sent: dict[str, float] = {}  # Track when warnings were sent
 157 | |         self.last_error_rate_check: float = time.time()
 158 | |
 159 | |         # Metric alert de-duplication (prevents alert spam on repeated updates)
 160 | |         self._last_metric_alert_level: dict[str, AlertLevel] = {}
 161 | |         self._last_metric_alert_time: dict[str, float] = {}
 162 | |         self._metric_alert_cooldown_seconds: float = 60.0  # Only re-log after 60s unless level escalates
 163 | |
 164 | |         # Performance optimization for long sessions
 165 | |         self._monitoring_interval: float = 30.0  # Base monitoring interval
 166 | |         self._adaptive_interval: bool = True  # Enable adaptive monitoring
 167 | |         self._last_cleanup_time: float = time.time()
 168 | |         self._cleanup_interval: float = 300.0  # Clean up every 5 minutes
 169 | |
 170 | |         # Automatic intervention flags
 171 | |         self._emergency_halt_requested: bool = False
 172 | |         self._emergency_halt_reason: str = ""
 173 | |         self._emergency_halt_timestamp: float = 0.0
 174 | |         self._immediate_intervention_requested: bool = False
 175 | |         self._immediate_intervention_reason: str = ""
 176 | |         self._immediate_intervention_timestamp: float = 0.0
 177 | |         self._enhanced_monitoring_active: bool = False
 178 | |         self._enhanced_monitoring_reason: str = ""
 179 | |         self._enhanced_monitoring_timestamp: float = 0.0
 180 | |
 181 | |         # Predictive analytics
 182 | |         self.failure_patterns: list[dict[str, Any]] = []
 183 | |         self.success_patterns: list[dict[str, Any]] = []
 184 | |         # Safety test mode flag to standardize alert prefixes
 185 | |         self._safety_test_mode: bool = False
 186 | |         # Integration bookkeeping
 187 | |         self._action6_callback_registered: bool = False
 188 | |
 189 | |         # Initialize metrics
 190 | |         self._initialize_metrics()
 191 | |
 192 | |         logger.debug("Session Health Monitor initialized")
 193 | |
 194 | |     def _initialize_metrics(self) -> None:
 195 | |         """Initialize health metrics with workload-appropriate thresholds for 724-page processing."""
 196 | |         metrics_config = {
 197 | |             "api_response_time": {"warning": 15.0, "critical": 25.0, "weight": 2.0},  # OPTIMIZATION: Less pessimistic thresholds (w‚Ä¶
 198 | |             "memory_usage_mb": {"warning": 200.0, "critical": 400.0, "weight": 1.5},
 199 | |             "error_rate": {"warning": 10.0, "critical": 25.0, "weight": 3.0},  # WORKLOAD-APPROPRIATE: Errors per hour for 724-page ‚Ä¶
 200 | |             "session_age_minutes": {"warning": 600.0, "critical": 1200.0, "weight": 1.0},  # WORKLOAD-APPROPRIATE: 10-20 hours for 7‚Ä¶
 201 | |             "browser_age_minutes": {"warning": 120.0, "critical": 180.0, "weight": 2.5},  # WORKLOAD-APPROPRIATE: 2-3 hours browser ‚Ä¶
 202 | |             "pages_since_refresh": {"warning": 50.0, "critical": 75.0, "weight": 2.0},  # WORKLOAD-APPROPRIATE: More pages before re‚Ä¶
 203 | |             "cpu_usage_percent": {"warning": 70.0, "critical": 90.0, "weight": 1.0},
 204 | |             "disk_usage_percent": {"warning": 85.0, "critical": 95.0, "weight": 0.5},
 205 | |         }
 206 | |
 207 | |         for name, config in metrics_config.items():
 208 | |             self.current_metrics[name] = HealthMetric(
 209 | |                 name=name,
 210 | |                 value=0.0,
 211 | |                 threshold_warning=config["warning"],
 212 | |                 threshold_critical=config["critical"],
 213 | |                 weight=config["weight"]
 214 | |             )
 215 | |             self.metrics_history[name] = deque(maxlen=100)
 216 | |
 217 | |     def begin_safety_test(self) -> None:
 218 | |         """Enable safety test mode to uniformly prefix all alerts and notices."""
 219 | |         self._safety_test_mode = True
 220 | |
 221 | |     def end_safety_test(self) -> None:
 222 | |         """Disable safety test mode."""
 223 | |         self._safety_test_mode = False
 224 | |
 225 | |     def _safety_prefix(self) -> str:
 226 | |         """Return the standard prefix for safety-test logs if enabled."""
 227 | |         return "üß™ [SAFETY TEST] " if self._safety_test_mode else ""
 228 | |
 229 | |     def update_metric(self, name: str, value: float):
 230 | |         """Update a specific health metric."""
 231 | |         with self.lock:
 232 | |             if name not in self.current_metrics:
 233 | |                 defaults = _DYNAMIC_METRIC_DEFAULTS.get(name)
 234 | |
 235 | |                 if defaults is None and name.startswith("api_") and name.endswith("_last_duration"):
 236 | |                     defaults = {"warning": 8.0, "critical": 15.0, "weight": 1.0}
 237 | |
 238 | |                 if defaults is not None:
 239 | |                     self.current_metrics[name] = HealthMetric(
 240 | |                         name=name,
 241 | |                         value=0.0,
 242 | |                         threshold_warning=defaults["warning"],
 243 | |                         threshold_critical=defaults["critical"],
 244 | |                         weight=defaults.get("weight", 1.0),
 245 | |                     )
 246 | |                     self.metrics_history[name] = deque(maxlen=100)
 247 | |                     logger.debug(f"Registered dynamic health metric: {name}")
 248 | |                 else:
 249 | |                     logger.debug(f"Ignoring update for unrecognized metric without defaults: {name}")
 250 | |                     return
 251 | |
 252 | |             metric = self.current_metrics[name]
 253 | |             metric.value = value
 254 | |             metric.timestamp = time.time()
 255 | |             self.metrics_history[name].append((time.time(), value))
 256 | |
 257 | |             # Check for alerts
 258 | |             self._check_metric_alerts(name)
 259 | |
 260 | |     def _check_metric_alerts(self, metric_name: str) -> None:
 261 | |         """Check if a metric triggers any alerts, with de-duplication and cooldown."""
 262 | |         metric = self.current_metrics[metric_name]
 263 | |
 264 | |         level: Optional[AlertLevel] = None
 265 | |         message: str = ""
 266 | |         threshold: float = 0.0
 267 | |
 268 | |         if metric.value >= metric.threshold_critical:
 269 | |             level = AlertLevel.CRITICAL
 270 | |             message = f"{metric_name} is critical: {metric.value:.2f} >= {metric.threshold_critical}"
 271 | |             threshold = metric.threshold_critical
 272 | |         elif metric.value >= metric.threshold_warning:
 273 | |             level = AlertLevel.WARNING
 274 | |             message = f"{metric_name} is elevated: {metric.value:.2f} >= {metric.threshold_warning}"
 275 | |             threshold = metric.threshold_warning
 276 | |
 277 | |         if level is None:
 278 | |             # Reset last level so future crossings log again
 279 | |             self._last_metric_alert_level.pop(metric_name, None)
 280 | |             return
 281 | |
 282 | |         # Gate logs: only log when escalating level or after cooldown
 283 | |         last_level = self._last_metric_alert_level.get(metric_name)
 284 | |         last_time = self._last_metric_alert_time.get(metric_name, 0.0)
 285 | |         now = time.time()
 286 | |
 287 | |         should_log = False
 288 | |         if last_level is None:
 289 | |             should_log = True
 290 | |         elif level.value != last_level.value:
 291 | |             # Escalation from WARNING->CRITICAL (or vice versa) should log
 292 | |             should_log = True
 293 | |         elif (now - last_time) >= self._metric_alert_cooldown_seconds:
 294 | |             should_log = True
 295 | |
 296 | |         if should_log:
 297 | |             self._create_alert(level, "session_health", message, metric_name, metric.value, threshold)
 298 | |             self._last_metric_alert_level[metric_name] = level
 299 | |             self._last_metric_alert_time[metric_name] = now
 300 | |
 301 | |     def _create_alert(self, level: AlertLevel, component: str, message: str,
 302 | |                      metric_name: str, metric_value: float, threshold: float):
 303 | |         """Create a new health alert."""
 304 | |         alert = HealthAlert(
 305 | |             level=level,
 306 | |             component=component,
 307 | |             message=message,
 308 | |             metric_name=metric_name,
 309 | |             metric_value=metric_value,
 310 | |             threshold=threshold
 311 | |         )
 312 | |
 313 | |         self.alerts.append(alert)
 314 | |
 315 | |         # Log alert with standardized safety test prefix when in test mode
 316 | |         test_prefix = self._safety_prefix()
 317 | |
 318 | |         if level == AlertLevel.CRITICAL:
 319 | |             logger.critical(f"{test_prefix}üö® CRITICAL ALERT: {message}")
 320 | |         elif level == AlertLevel.WARNING:
 321 | |             logger.warning(f"{test_prefix}‚ö†Ô∏è WARNING: {message}")
 322 | |         else:
 323 | |             logger.info(f"{test_prefix}INFO: {message}")
 324 | |
 325 | |     def calculate_health_score(self) -> float:
 326 | |         """
 327 | |         Calculate overall health score (0-100) based on all metrics.
 328 | |
 329 | |         Returns:
 330 | |             Health score from 0 (critical) to 100 (excellent)
 331 | |         """
 332 | |         with self.lock:
 333 | |             total_score = 100.0
 334 | |             total_weight = 0.0
 335 | |
 336 | |             for metric in self.current_metrics.values():
 337 | |                 # Calculate metric score (0-100)
 338 | |                 if metric.value <= metric.threshold_warning * 0.5:
 339 | |                     metric_score = 100.0  # Excellent
 340 | |                 elif metric.value <= metric.threshold_warning * 0.8:
 341 | |                     metric_score = 80.0   # Good
 342 | |                 elif metric.value <= metric.threshold_warning:
 343 | |                     metric_score = 60.0   # Fair
 344 | |                 elif metric.value <= metric.threshold_critical:
 345 | |                     metric_score = 30.0   # Poor
 346 | |                 else:
 347 | |                     metric_score = 0.0    # Critical
 348 | |
 349 | |                 # Apply weighted average
 350 | |                 total_score += metric_score * metric.weight
 351 | |                 total_weight += metric.weight
 352 | |
 353 | |             # Calculate final weighted score
 354 | |             final_score = total_score / total_weight if total_weight > 0 else 0.0
 355 | |
 356 | |             # Store in history
 357 | |             self.health_score_history.append((time.time(), final_score))
 358 | |
 359 | |             return max(0.0, min(100.0, final_score))
 360 | |
 361 | |     def get_health_status(self) -> HealthStatus:
 362 | |         """Get overall health status based on current score."""
 363 | |         score = self.calculate_health_score()
 364 | |
 365 | |         if score >= 80:
 366 | |             return HealthStatus.EXCELLENT
 367 | |         if score >= 60:
 368 | |             return HealthStatus.GOOD
 369 | |         if score >= 40:
 370 | |             return HealthStatus.FAIR
 371 | |         if score >= 20:
 372 | |             return HealthStatus.POOR
 373 | |         return HealthStatus.CRITICAL
 374 | |
 375 | |     def _calculate_api_response_risk(self) -> float:
 376 | |         """Calculate risk from API response time trend."""
 377 | |         if len(self.api_response_times) < 3:
 378 | |             return 0.0
 379 | |
 380 | |         recent_avg = sum(list(self.api_response_times)[-3:]) / 3
 381 | |         if recent_avg > 10.0:
 382 | |             return 0.4
 383 | |         if recent_avg > 8.0:
 384 | |             return 0.3
 385 | |         if recent_avg > 5.0:
 386 | |             return 0.2
 387 | |         return 0.0
 388 | |
 389 | |     def _calculate_error_rate_risk(self) -> float:
 390 | |         """Calculate risk from error rate."""
 391 | |         total_errors = sum(self.error_counts.values())
 392 | |         if total_errors > 15:
 393 | |             return 0.4
 394 | |         if total_errors > 10:
 395 | |             return 0.3
 396 | |         if total_errors > 5:
 397 | |             return 0.2
 398 | |         return 0.0
 399 | |
 400 | |     def _calculate_memory_trend_risk(self) -> float:
 401 | |         """Calculate risk from memory usage trend."""
 402 | |         if len(self.memory_usage_history) < 3:
 403 | |             return 0.0
 404 | |
 405 | |         memory_trend = list(self.memory_usage_history)[-1] - list(self.memory_usage_history)[-3]
 406 | |         if memory_trend > 100:
 407 | |             return 0.2
 408 | |         if memory_trend > 50:
 409 | |             return 0.1
 410 | |         return 0.0
 411 | |
 412 | |     def _calculate_critical_metrics_risk(self) -> float:
 413 | |         """Calculate risk from critical metrics."""
 414 | |         risk = 0.0
 415 | |         for _, metric in self.current_metrics.items():
 416 | |             if metric.value >= metric.threshold_critical:
 417 | |                 risk += 0.15
 418 | |             elif metric.value >= metric.threshold_warning:
 419 | |                 risk += 0.05
 420 | |         return risk
 421 | |
 422 | |     def predict_session_death_risk(self) -> float:
 423 | |         """
 424 | |         Enhanced prediction of session death likelihood in next 10 pages (0.0-1.0).
 425 | |
 426 | |         Returns:
 427 | |             Risk score from 0.0 (very safe) to 1.0 (imminent failure)
 428 | |         """
 429 | |         # Base risk from current health score
 430 | |         health_score = self.calculate_health_score()
 431 | |         health_risk = (100 - health_score) / 100 * 0.5
 432 | |
 433 | |         # Aggregate all risk factors
 434 | |         risk_score = (
 435 | |             health_risk +
 436 | |             self._calculate_api_response_risk() +
 437 | |             self._calculate_error_rate_risk() +
 438 | |             self._calculate_memory_trend_risk() +
 439 | |             self._calculate_critical_metrics_risk()
 440 | |         )
 441 | |
 442 | |         return min(1.0, risk_score)
 443 | |
 444 | |     def get_recommended_actions(self) -> list[str]:
 445 | |         """Get recommended actions based on current health status."""
 446 | |         actions: list[str] = []
 447 | |         health_score = self.calculate_health_score()
 448 | |         risk_score = self.predict_session_death_risk()
 449 | |
 450 | |         if risk_score > 0.8:
 451 | |             actions.append("üö® EMERGENCY: Trigger immediate session refresh")
 452 | |             actions.append("üîÑ Restart browser immediately")
 453 | |             actions.append("‚ö° Switch to emergency settings (1 worker, batch 1)")
 454 | |         elif risk_score > 0.6:
 455 | |             actions.append("‚ö†Ô∏è CRITICAL: Reduce concurrency to 1 worker")
 456 | |             actions.append("üìâ Reduce batch size to 3")
 457 | |             actions.append("üîÑ Schedule session refresh within 5 pages")
 458 | |         elif risk_score > 0.4:
 459 | |             actions.append("‚ö†Ô∏è WARNING: Reduce batch size to 5")
 460 | |             actions.append("üìä Monitor closely for next 10 pages")
 461 | |             actions.append("üîÑ Consider session refresh within 15 pages")
 462 | |         elif health_score < 60:
 463 | |             actions.append("üìä Monitor performance metrics")
 464 | |             actions.append("üßπ Consider garbage collection")
 465 | |         else:
 466 | |             actions.append("‚úÖ System healthy - continue current operations")
 467 | |
 468 | |         return actions
 469 | |
 470 | |     def record_api_response_time(self, response_time: float) -> None:
 471 | |         """Record API response time for monitoring."""
 472 | |         self.api_response_times.append(response_time)
 473 | |
 474 | |         # Update metric
 475 | |         if len(self.api_response_times) >= 5:
 476 | |             avg_response_time = sum(list(self.api_response_times)[-5:]) / 5
 477 | |             self.update_metric("api_response_time", avg_response_time)
 478 | |
 479 | |     def record_error(self, error_type: str) -> None:
 480 | |         """Record an error for monitoring with enhanced rate tracking."""
 481 | |         current_time = time.time()
 482 | |
 483 | |         if error_type not in self.error_counts:
 484 | |             self.error_counts[error_type] = 0
 485 | |         self.error_counts[error_type] += 1
 486 | |
 487 | |         # Track error timestamp for rate analysis
 488 | |         self.error_timestamps.append(current_time)
 489 | |
 490 | |         # Update error rate metric
 491 | |         total_errors = sum(self.error_counts.values())
 492 | |         session_duration_hours = (current_time - self.session_start_time) / 3600
 493 | |         error_rate = total_errors / max(session_duration_hours, 0.1)  # Errors per hour
 494 | |         self.update_metric("error_rate", error_rate)
 495 | |
 496 | |         # Check for early warning conditions
 497 | |         self._check_error_rate_early_warning(current_time)
 498 | |
 499 | |     def _process_error_window_threshold(
 500 | |         self,
 501 | |         window_name: str,
 502 | |         errors_in_window: int,
 503 | |         threshold: int,
 504 | |         current_time: float
 505 | |     ) -> None:
 506 | |         """Process error threshold breach for a specific time window."""
 507 | |         warning_key = f"{window_name}_{threshold}"
 508 | |
 509 | |         # Only send warning once per hour for each threshold
 510 | |         last_warning = self.error_rate_warnings_sent.get(warning_key, 0)
 511 | |         if current_time - last_warning <= 3600:  # 1 hour
 512 | |             return
 513 | |
 514 | |         self.error_rate_warnings_sent[warning_key] = current_time
 515 | |
 516 | |         # Create critical alert
 517 | |         alert = HealthAlert(
 518 | |             level=AlertLevel.CRITICAL,
 519 | |             component="error_rate_monitor",
 520 | |             message=f"üö® HIGH ERROR RATE: {errors_in_window} errors in {window_name} (threshold: {threshold})",
 521 | |             metric_name="error_rate_early_warning",
 522 | |             metric_value=errors_in_window,
 523 | |             threshold=threshold,
 524 | |             timestamp=current_time
 525 | |         )
 526 | |
 527 | |         self.alerts.append(alert)
 528 | |         prefix = self._safety_prefix()
 529 | |         logger.critical(f"{prefix}üö® CRITICAL ALERT: {alert.message}")
 530 | |
 531 | |         # WORKLOAD-APPROPRIATE: Cascade failure detection with automatic intervention
 532 | |         if window_name == "30-minute" and errors_in_window >= 500:
 533 | |             logger.critical(f"{prefix}üö® CASCADE FAILURE DETECTED - EMERGENCY INTERVENTION REQUIRED")
 534 | |             self._trigger_emergency_intervention("CASCADE_FAILURE", errors_in_window, window_name)
 535 | |         elif window_name == "15-minute" and errors_in_window >= 200:
 536 | |             logger.critical(f"{prefix}üö® SEVERE ERROR PATTERN DETECTED - Triggering immediate intervention")
 537 | |             self._trigger_immediate_intervention("SEVERE_ERROR_PATTERN", errors_in_window, window_name)
 538 | |         elif window_name == "5-minute" and errors_in_window >= 75:
 539 | |             logger.debug(f"{prefix}‚ö†Ô∏è ELEVATED ERROR RATE - Triggering enhanced monitoring")
 540 | |             self._trigger_enhanced_monitoring("ELEVATED_ERROR_RATE", errors_in_window, window_name)
 541 | |
 542 | |     def _check_error_rate_early_warning(self, current_time: float) -> None:
 543 | |         """
 544 | |         PERFORMANCE-OPTIMIZED error rate monitoring for long sessions.
 545 | |
 546 | |         Uses adaptive monitoring intervals and efficient cleanup to reduce overhead.
 547 | |         """
 548 | |         # Adaptive monitoring interval based on current error rate
 549 | |         monitoring_interval = self._get_adaptive_monitoring_interval(current_time)
 550 | |
 551 | |         # Only check at adaptive intervals to reduce overhead
 552 | |         if current_time - self.last_error_rate_check < monitoring_interval:
 553 | |             return
 554 | |
 555 | |         self.last_error_rate_check = current_time
 556 | |
 557 | |         # PERFORMANCE: Efficient cleanup with batched operations
 558 | |         self._perform_efficient_cleanup(current_time)
 559 | |
 560 | |         # Check different time windows for early warning - WORKLOAD-APPROPRIATE for 724 pages
 561 | |         time_windows = [
 562 | |             (1800, 500, "30-minute"),  # 500 errors in 30 minutes (cascade failure)
 563 | |             (900, 200, "15-minute"),   # 200 errors in 15 minutes (severe issues)
 564 | |             (300, 75, "5-minute"),     # 75 errors in 5 minutes (moderate issues)
 565 | |             (60, 15, "1-minute"),      # 15 errors in 1 minute (immediate issues)
 566 | |         ]
 567 | |
 568 | |         for window_seconds, threshold, window_name in time_windows:
 569 | |             window_start = current_time - window_seconds
 570 | |             errors_in_window = sum(1 for ts in self.error_timestamps if ts >= window_start)
 571 | |
 572 | |             if errors_in_window >= threshold:
 573 | |                 self._process_error_window_threshold(window_name, errors_in_window, threshold, current_time)
 574 | |
 575 | |     def get_error_rate_statistics(self) -> dict[str, Any]:
 576 | |         """
 577 | |         Get comprehensive error rate statistics for monitoring and analysis.
 578 | |
 579 | |         Returns:
 580 | |             Dict containing error rate statistics and recommendations
 581 | |         """
 582 | |         current_time = time.time()
 583 | |
 584 | |         # Calculate error rates for different time windows
 585 | |         time_windows = [
 586 | |             (60, "1-minute"),
 587 | |             (300, "5-minute"),
 588 | |             (900, "15-minute"),
 589 | |             (1800, "30-minute"),
 590 | |             (3600, "1-hour"),
 591 | |             (7200, "2-hour")
 592 | |         ]
 593 | |
 594 | |         error_rates = {}
 595 | |         for window_seconds, window_name in time_windows:
 596 | |             window_start = current_time - window_seconds
 597 | |             errors_in_window = sum(1 for ts in self.error_timestamps if ts >= window_start)
 598 | |             error_rates[window_name] = {
 599 | |                 "count": errors_in_window,
 600 | |                 "rate_per_minute": errors_in_window / (window_seconds / 60),
 601 | |                 "window_seconds": window_seconds
 602 | |             }
 603 | |
 604 | |         # Determine risk level - WORKLOAD-APPROPRIATE for 724-page processing
 605 | |         thirty_min_errors = error_rates["30-minute"]["count"]
 606 | |         fifteen_min_errors = error_rates["15-minute"]["count"]
 607 | |         five_min_errors = error_rates["5-minute"]["count"]
 608 | |
 609 | |         if thirty_min_errors >= 500:
 610 | |             risk_level = "CRITICAL"
 611 | |             recommendation = "EMERGENCY_SHUTDOWN"
 612 | |         elif fifteen_min_errors >= 200:
 613 | |             risk_level = "HIGH"
 614 | |             recommendation = "IMMEDIATE_INTERVENTION"
 615 | |         elif five_min_errors >= 75:
 616 | |             risk_level = "MODERATE"
 617 | |             recommendation = "MONITOR_CLOSELY"
 618 | |         elif five_min_errors >= 25:
 619 | |             risk_level = "ELEVATED"
 620 | |             recommendation = "INCREASED_MONITORING"
 621 | |         else:
 622 | |             risk_level = "LOW"
 623 | |             recommendation = "CONTINUE_NORMAL"
 624 | |
 625 | |         return {
 626 | |             "timestamp": current_time,
 627 | |             "error_rates": error_rates,
 628 | |             "total_session_errors": sum(self.error_counts.values()),
 629 | |             "session_duration_minutes": (current_time - self.session_start_time) / 60,
 630 | |             "risk_level": risk_level,
 631 | |             "recommendation": recommendation,
 632 | |             "error_types": dict(self.error_counts),
 633 | |             "recent_alerts": [alert for alert in self.alerts if current_time - alert.timestamp < 3600]
 634 | |         }
 635 | |
 636 | |     def _trigger_emergency_intervention(self, pattern_type: str, error_count: int, window: str) -> None:
 637 | |         """Trigger emergency intervention for cascade failures."""
 638 | |         try:
 639 | |             prefix = self._safety_prefix()
 640 | |             logger.critical(f"{prefix}üö® EMERGENCY INTERVENTION TRIGGERED: {pattern_type}")
 641 | |             logger.critical(f"{prefix}   Pattern: {error_count} errors in {window}")
 642 | |             logger.critical(f"{prefix}   Action: Setting emergency halt flag")
 643 | |
 644 | |             # Set emergency halt flag that can be checked by main processing loops
 645 | |             self._emergency_halt_requested = True
 646 | |             self._emergency_halt_reason = f"{pattern_type}: {error_count} errors in {window}"
 647 | |             self._emergency_halt_timestamp = time.time()
 648 | |
 649 | |             # Create critical alert
 650 | |             alert = HealthAlert(
 651 | |                 level=AlertLevel.CRITICAL,
 652 | |                 component="emergency_intervention",
 653 | |                 message=f"üö® EMERGENCY HALT: {pattern_type} - {error_count} errors in {window}",
 654 | |                 metric_name="emergency_intervention",
 655 | |                 metric_value=error_count,
 656 | |                 threshold=500 if window == "30-minute" else 200,
 657 | |                 timestamp=time.time()
 658 | |             )
 659 | |             self.alerts.append(alert)
 660 | |
 661 | |             logger.critical(f"{self._safety_prefix()}üö® EMERGENCY INTERVENTION COMPLETE - Processing should halt immediately")
 662 | |
 663 | |         except Exception as e:
 664 | |             logger.error(f"Failed to trigger emergency intervention: {e}")
 665 | |
 666 | |     def _trigger_immediate_intervention(self, pattern_type: str, error_count: int, window: str) -> None:
 667 | |         """Trigger immediate intervention for severe error patterns."""
 668 | |         try:
 669 | |             prefix = self._safety_prefix()
 670 | |             logger.critical(f"{prefix}‚ö†Ô∏è IMMEDIATE INTERVENTION TRIGGERED: {pattern_type}")
 671 | |             logger.critical(f"{prefix}   Pattern: {error_count} errors in {window}")
 672 | |             logger.critical(f"{prefix}   Action: Setting immediate halt flag and triggering recovery")
 673 | |
 674 | |             # Set immediate intervention flag
 675 | |             self._immediate_intervention_requested = True
 676 | |             self._immediate_intervention_reason = f"{pattern_type}: {error_count} errors in {window}"
 677 | |             self._immediate_intervention_timestamp = time.time()
 678 | |
 679 | |             # Create critical alert
 680 | |             alert = HealthAlert(
 681 | |                 level=AlertLevel.CRITICAL,
 682 | |                 component="immediate_intervention",
 683 | |                 message=f"‚ö†Ô∏è IMMEDIATE INTERVENTION: {pattern_type} - {error_count} errors in {window}",
 684 | |                 metric_name="immediate_intervention",
 685 | |                 metric_value=error_count,
 686 | |                 threshold=200 if window == "15-minute" else 75,
 687 | |                 timestamp=time.time()
 688 | |             )
 689 | |             self.alerts.append(alert)
 690 | |
 691 | |             logger.critical(f"{self._safety_prefix()}‚ö†Ô∏è IMMEDIATE INTERVENTION COMPLETE - Consider halting or recovery")
 692 | |
 693 | |         except Exception as e:
 694 | |             logger.error(f"Failed to trigger immediate intervention: {e}")
 695 | |
 696 | |     def _trigger_enhanced_monitoring(self, pattern_type: str, error_count: int, window: str) -> None:
 697 | |         """Trigger enhanced monitoring for elevated error rates."""
 698 | |         try:
 699 | |             prefix = self._safety_prefix()
 700 | |             logger.debug(f"{prefix}üìä ENHANCED MONITORING TRIGGERED: {pattern_type}")
 701 | |             logger.debug(f"{prefix}   Pattern: {error_count} errors in {window}")
 702 | |             logger.debug(f"{prefix}   Action: Increasing monitoring frequency")
 703 | |
 704 | |             # Set enhanced monitoring flag
 705 | |             self._enhanced_monitoring_active = True
 706 | |             self._enhanced_monitoring_reason = f"{pattern_type}: {error_count} errors in {window}"
 707 | |             self._enhanced_monitoring_timestamp = time.time()
 708 | |
 709 | |             # Reduce monitoring interval for enhanced monitoring
 710 | |             self.last_error_rate_check = time.time() - 25  # Check again in 5 seconds instead of 30
 711 | |
 712 | |             # Create warning alert
 713 | |             alert = HealthAlert(
 714 | |                 level=AlertLevel.WARNING,
 715 | |                 component="enhanced_monitoring",
 716 | |                 message=f"üìä ENHANCED MONITORING: {pattern_type} - {error_count} errors in {window}",
 717 | |                 metric_name="enhanced_monitoring",
 718 | |                 metric_value=error_count,
 719 | |                 threshold=75,
 720 | |                 timestamp=time.time()
 721 | |             )
 722 | |             self.alerts.append(alert)
 723 | |
 724 | |             logger.debug(f"{self._safety_prefix()}üìä ENHANCED MONITORING ACTIVE - Increased error rate surveillance")
 725 | |
 726 | |         except Exception as e:
 727 | |             logger.error(f"Failed to trigger enhanced monitoring: {e}")
 728 | |
 729 | |     def should_emergency_halt(self) -> bool:
 730 | |         """Check if emergency halt has been requested."""
 731 | |         return self._emergency_halt_requested
 732 | |
 733 | |     def should_immediate_intervention(self) -> bool:
 734 | |         """Check if immediate intervention has been requested."""
 735 | |         return self._immediate_intervention_requested
 736 | |
 737 | |     def is_enhanced_monitoring_active(self) -> bool:
 738 | |         """Check if enhanced monitoring is active."""
 739 | |         return self._enhanced_monitoring_active
 740 | |
 741 | |     def get_intervention_status(self) -> dict[str, Any]:
 742 | |         """Get current intervention status."""
 743 | |         return {
 744 | |             "emergency_halt": {
 745 | |                 "requested": self._emergency_halt_requested,
 746 | |                 "reason": self._emergency_halt_reason,
 747 | |                 "timestamp": self._emergency_halt_timestamp
 748 | |             },
 749 | |             "immediate_intervention": {
 750 | |                 "requested": self._immediate_intervention_requested,
 751 | |                 "reason": self._immediate_intervention_reason,
 752 | |                 "timestamp": self._immediate_intervention_timestamp
 753 | |             },
 754 | |             "enhanced_monitoring": {
 755 | |                 "active": self._enhanced_monitoring_active,
 756 | |                 "reason": self._enhanced_monitoring_reason,
 757 | |                 "timestamp": self._enhanced_monitoring_timestamp
 758 | |             }
 759 | |         }
 760 | |
 761 | |     def reset_intervention_flags(self) -> None:
 762 | |         """Reset intervention flags (use with caution)."""
 763 | |         logger.debug("üîÑ Resetting intervention flags")
 764 | |         self._emergency_halt_requested = False
 765 | |         self._emergency_halt_reason = ""
 766 | |         self._emergency_halt_timestamp = 0.0
 767 | |         self._immediate_intervention_requested = False
 768 | |         self._immediate_intervention_reason = ""
 769 | |         self._immediate_intervention_timestamp = 0.0
 770 | |         self._enhanced_monitoring_active = False
 771 | |         self._enhanced_monitoring_reason = ""
 772 | |         self._enhanced_monitoring_timestamp = 0.0
 773 | |
 774 | |     def _get_adaptive_monitoring_interval(self, current_time: float) -> float:
 775 | |         """Get adaptive monitoring interval based on current system state."""
 776 | |         try:
 777 | |             # Base interval
 778 | |             interval = self._monitoring_interval
 779 | |
 780 | |             # If enhanced monitoring is active, check more frequently
 781 | |             if self._enhanced_monitoring_active:
 782 | |                 interval = 5.0  # Check every 5 seconds during enhanced monitoring
 783 | |             else:
 784 | |                 # Adaptive interval based on recent error rate
 785 | |                 recent_errors = sum(1 for ts in self.error_timestamps if current_time - ts < 300)  # Last 5 minutes
 786 | |
 787 | |                 if recent_errors >= 50:
 788 | |                     interval = 10.0  # High error rate - check every 10 seconds
 789 | |                 elif recent_errors >= 20:
 790 | |                     interval = 20.0  # Moderate error rate - check every 20 seconds
 791 | |                 elif recent_errors >= 5:
 792 | |                     interval = 30.0  # Low error rate - normal interval
 793 | |                 else:
 794 | |                     interval = 60.0  # Very low error rate - check every minute
 795 | |
 796 | |             return interval
 797 | |
 798 | |         except Exception as e:
 799 | |             logger.debug(f"Error calculating adaptive interval: {e}")
 800 | |             return self._monitoring_interval
 801 | |
 802 | |     def _perform_efficient_cleanup(self, current_time: float) -> None:
 803 | |         """Perform efficient cleanup of old data to reduce memory usage."""
 804 | |         try:
 805 | |             # Only perform cleanup every 5 minutes to reduce overhead
 806 | |             if current_time - self._last_cleanup_time < self._cleanup_interval:
 807 | |                 return
 808 | |
 809 | |             self._last_cleanup_time = current_time
 810 | |
 811 | |             # Clean old error timestamps (older than 2 hours for long sessions)
 812 | |             cutoff_time = current_time - 7200  # 2 hours
 813 | |
 814 | |             # PERFORMANCE: Batch cleanup instead of one-by-one
 815 | |             cleanup_count = 0
 816 | |             while self.error_timestamps and self.error_timestamps[0] < cutoff_time:
 817 | |                 self.error_timestamps.popleft()
 818 | |                 cleanup_count += 1
 819 | |
 820 | |                 # Prevent excessive cleanup in one operation
 821 | |                 if cleanup_count > 1000:
 822 | |                     break
 823 | |
 824 | |             # Clean old warning timestamps
 825 | |             warning_cutoff = current_time - 3600  # 1 hour
 826 | |             old_warnings = [key for key, timestamp in self.error_rate_warnings_sent.items()
 827 | |                           if timestamp < warning_cutoff]
 828 | |             for key in old_warnings:
 829 | |                 del self.error_rate_warnings_sent[key]
 830 | |
 831 | |             # Clean old alerts (keep only last 4 hours)
 832 | |             alert_cutoff = current_time - 14400  # 4 hours
 833 | |             self.alerts = [alert for alert in self.alerts if alert.timestamp >= alert_cutoff]
 834 | |
 835 | |             if cleanup_count > 0:
 836 | |                 logger.debug(f"Cleaned up {cleanup_count} old error timestamps and {len(old_warnings)} old warnings")
 837 | |
 838 | |         except Exception as e:
 839 | |             logger.debug(f"Error during efficient cleanup: {e}")
 840 | |
 841 | |     def optimize_for_long_session(self) -> None:
 842 | |         """Optimize monitoring settings for long-running sessions (20+ hours)."""
 843 | |         try:
 844 | |             logger.info("üîß Optimizing health monitoring for long session")
 845 | |
 846 | |             # Increase monitoring intervals to reduce overhead
 847 | |             self._monitoring_interval = 60.0  # Check every minute instead of 30 seconds
 848 | |             self._cleanup_interval = 600.0    # Clean up every 10 minutes instead of 5
 849 | |
 850 | |             # Increase deque sizes for longer data retention
 851 | |             if self.error_timestamps.maxlen is None or self.error_timestamps.maxlen < 3000:
 852 | |                 # Create new deque with larger size and copy existing data
 853 | |                 old_timestamps = list(self.error_timestamps)
 854 | |                 self.error_timestamps = deque(old_timestamps, maxlen=3000)
 855 | |
 856 | |             # Optimize metrics history for longer sessions
 857 | |             for name in self.metrics_history:
 858 | |                 current_maxlen = self.metrics_history[name].maxlen
 859 | |                 if current_maxlen is None or current_maxlen < 200:
 860 | |                     old_history = list(self.metrics_history[name])
 861 | |                     self.metrics_history[name] = deque(old_history, maxlen=200)
 862 | |
 863 | |             logger.info("‚úÖ Health monitoring optimized for long session")
 864 | |
 865 | |         except Exception as e:
 866 | |             logger.debug(f"Failed to optimize for long session: {e}")
 867 | |
 868 | |     def get_performance_stats(self) -> dict[str, Any]:
 869 | |         """Get performance statistics for monitoring overhead analysis."""
 870 | |         try:
 871 | |             current_time = time.time()
 872 | |             session_duration = (current_time - self.session_start_time) / 3600  # Hours
 873 | |
 874 | |             return {
 875 | |                 "session_duration_hours": session_duration,
 876 | |                 "error_timestamps_count": len(self.error_timestamps),
 877 | |                 "error_timestamps_max": self.error_timestamps.maxlen,
 878 | |                 "alerts_count": len(self.alerts),
 879 | |                 "warnings_sent_count": len(self.error_rate_warnings_sent),
 880 | |                 "current_monitoring_interval": self._monitoring_interval,
 881 | |                 "adaptive_monitoring_enabled": self._adaptive_interval,
 882 | |                 "last_cleanup_age_minutes": (current_time - self._last_cleanup_time) / 60,
 883 | |                 "memory_efficiency": {
 884 | |                     "error_timestamps_usage": f"{len(self.error_timestamps)}/{self.error_timestamps.maxlen}",
 885 | |                     "metrics_history_total": sum(len(hist) for hist in self.metrics_history.values()),
 886 | |                     "alerts_retention_hours": 4,
 887 | |                     "warnings_retention_hours": 1
 888 | |                 }
 889 | |             }
 890 | |
 891 | |         except Exception as e:
 892 | |             logger.debug(f"Error getting performance stats: {e}")
 893 | |             return {"error": str(e)}
 894 | |
 895 | |     def record_page_processing_time(self, processing_time: float) -> None:
 896 | |         """Record page processing time."""
 897 | |         self.page_processing_times.append(processing_time)
 898 | |
 899 | |     def update_system_metrics(self) -> None:
 900 | |         """Update system-level metrics (CPU, memory, etc.)."""
 901 | |         try:
 902 | |             # Memory usage
 903 | |             process = psutil.Process()
 904 | |             memory_mb = process.memory_info().rss / 1024 / 1024
 905 | |             self.memory_usage_history.append(memory_mb)
 906 | |             self.update_metric("memory_usage_mb", memory_mb)
 907 | |
 908 | |             # CPU usage
 909 | |             cpu_percent = psutil.cpu_percent(interval=0.1)
 910 | |             self.update_metric("cpu_usage_percent", cpu_percent)
 911 | |
 912 | |             # Disk usage
 913 | |             disk_usage = psutil.disk_usage('/').percent
 914 | |             self.update_metric("disk_usage_percent", disk_usage)
 915 | |
 916 | |         except Exception as e:
 917 | |             logger.debug(f"Error updating system metrics: {e}")
 918 | |
 919 | |     def _update_browser_health_metrics(self, monitor: Any) -> None:
 920 | |         """Update browser health metrics from monitor."""
 921 | |         if not (hasattr(monitor, 'get') or isinstance(monitor, dict)):
 922 | |             return
 923 | |
 924 | |         monitor_dict = cast(dict[str, Any], monitor)
 925 | |
 926 | |         # Browser age
 927 | |         browser_start_time = monitor_dict.get('browser_start_time', time.time())
 928 | |         if browser_start_time:
 929 | |             browser_age_minutes = (time.time() - browser_start_time) / 60
 930 | |             self.update_metric("browser_age_minutes", browser_age_minutes)
 931 | |
 932 | |         # Pages since refresh
 933 | |         pages_since_refresh = monitor_dict.get('pages_since_refresh', 0)
 934 | |         if pages_since_refresh is not None:
 935 | |             self.update_metric("pages_since_refresh", pages_since_refresh)
 936 | |
 937 | |     def _update_session_health_metrics(self, session_monitor: Any) -> None:
 938 | |         """Update session health metrics from monitor."""
 939 | |         if not (hasattr(session_monitor, 'get') or isinstance(session_monitor, dict)):
 940 | |             return
 941 | |
 942 | |         session_monitor_dict = cast(dict[str, Any], session_monitor)
 943 | |         session_start = session_monitor_dict.get('session_start_time', time.time())
 944 | |         if session_start:
 945 | |             session_age_minutes = (time.time() - session_start) / 60
 946 | |             self.update_metric("session_age_minutes", session_age_minutes)
 947 | |
 948 | |     def update_session_metrics(self, session_manager: Any = None) -> None:
 949 | |         """Update session-specific metrics with enhanced error handling."""
 950 | |         try:
 951 | |             # Session age
 952 | |             session_age_minutes = (time.time() - self.session_start_time) / 60
 953 | |             self.update_metric("session_age_minutes", session_age_minutes)
 954 | |
 955 | |             if session_manager:
 956 | |                 # Handle browser health monitor
 957 | |                 if hasattr(session_manager, 'browser_health_monitor'):
 958 | |                     try:
 959 | |                         self._update_browser_health_metrics(session_manager.browser_health_monitor)
 960 | |                     except Exception as browser_exc:
 961 | |                         logger.debug(f"Browser health monitor update failed: {browser_exc}")
 962 | |
 963 | |                 # Handle session health monitor
 964 | |                 if hasattr(session_manager, 'session_health_monitor'):
 965 | |                     try:
 966 | |                         self._update_session_health_metrics(session_manager.session_health_monitor)
 967 | |                     except Exception as session_exc:
 968 | |                         logger.debug(f"Session health monitor update failed: {session_exc}")
 969 | |
 970 | |         except Exception as e:
 971 | |             logger.debug(f"Error updating session metrics: {e}")
 972 | |
 973 | |     def get_health_dashboard(self) -> dict[str, Any]:
 974 | |         """Get comprehensive health dashboard data."""
 975 | |         health_score = self.calculate_health_score()
 976 | |         health_status = self.get_health_status()
 977 | |         risk_score = self.predict_session_death_risk()
 978 | |
 979 | |         return {
 980 | |             "timestamp": time.time(),
 981 | |             "health_score": health_score,
 982 | |             "health_status": health_status.value,
 983 | |             "risk_score": risk_score,
 984 | |             "risk_level": self._get_risk_level(risk_score),
 985 | |             "metrics": {name: {
 986 | |                 "value": metric.value,
 987 | |                 "status": metric.status.value,
 988 | |                 "threshold_warning": metric.threshold_warning,
 989 | |                 "threshold_critical": metric.threshold_critical
 990 | |             } for name, metric in self.current_metrics.items()},
 991 | |             "recent_alerts": [
 992 | |                 {
 993 | |                     "level": alert.level.value,
 994 | |                     "component": alert.component,
 995 | |                     "message": alert.message,
 996 | |                     "timestamp": alert.timestamp
 997 | |                 } for alert in self.alerts[-5:]  # Last 5 alerts
 998 | |             ],
 999 | |             "recommended_actions": self.get_recommended_actions(),
1000 | |             "performance_summary": {
1001 | |                 "avg_api_response_time": sum(self.api_response_times) / len(self.api_response_times) if self.api_response_times else‚Ä¶
1002 | |                 "total_errors": sum(self.error_counts.values()),
1003 | |                 "avg_page_processing_time": sum(self.page_processing_times) / len(self.page_processing_times) if self.page_processin‚Ä¶
1004 | |                 "current_memory_mb": self.memory_usage_history[-1] if self.memory_usage_history else 0
1005 | |             }
1006 | |         }
1007 | |
1008 | |     @staticmethod
1009 | |     def _get_risk_level(risk_score: float) -> str:
1010 | |         """Convert risk score to human-readable level."""
1011 | |         if risk_score > 0.8:
1012 | |             return "EMERGENCY"
1013 | |         if risk_score > 0.6:
1014 | |             return "CRITICAL"
1015 | |         if risk_score > 0.4:
1016 | |             return "WARNING"
1017 | |         if risk_score > 0.2:
1018 | |             return "CAUTION"
1019 | |         return "SAFE"
1020 | |
1021 | |     def log_health_summary(self) -> None:
1022 | |         """Log a comprehensive health summary."""
1023 | |         dashboard = self.get_health_dashboard()
1024 | |
1025 | |         logger.info("üìä HEALTH SUMMARY:")
1026 | |         logger.info(f"   Score: {dashboard['health_score']:.1f}/100 ({dashboard['health_status'].upper()})")
1027 | |         logger.info(f"   Risk: {dashboard['risk_score']:.2f} ({dashboard['risk_level']})")
1028 | |         logger.info(f"   API: {dashboard['performance_summary']['avg_api_response_time']:.1f}s avg")
1029 | |         logger.info(f"   Memory: {dashboard['performance_summary']['current_memory_mb']:.1f}MB")
1030 | |         logger.info(f"   Errors: {dashboard['performance_summary']['total_errors']}")
1031 | |
1032 | |         if dashboard['recommended_actions']:
1033 | |             logger.info(f"   Actions: {dashboard['recommended_actions'][0]}")
1034 | |
1035 | |     # === SESSION STATE PERSISTENCE ===
1036 | |
1037 | |     def create_session_checkpoint(self, checkpoint_name: Optional[str] = None) -> str:
1038 | |         """Create a checkpoint of the current session state for recovery."""
1039 | |         try:
1040 | |             if checkpoint_name is None:
1041 | |                 checkpoint_name = f"checkpoint_{int(time.time())}"
1042 | |
1043 | |             # Create checkpoint directory
1044 | |             checkpoint_dir = Path("Cache/session_checkpoints")
1045 | |             checkpoint_dir.mkdir(parents=True, exist_ok=True)
1046 | |
1047 | |             # Prepare session state data
1048 | |             session_state = {
1049 | |                 "timestamp": time.time(),
1050 | |                 "checkpoint_name": checkpoint_name,
1051 | |                 "session_start_time": self.session_start_time,
1052 | |                 "health_score_history": list(self.health_score_history),
1053 | |                 "current_metrics": {
1054 | |                     name: {
1055 | |                         "value": metric.value,
1056 | |                         "status": metric.status.value,
1057 | |                         "threshold_warning": metric.threshold_warning,
1058 | |                         "threshold_critical": metric.threshold_critical,
1059 | |                         "timestamp": metric.timestamp
1060 | |                     } for name, metric in self.current_metrics.items()
1061 | |                 },
1062 | |                 "alerts": [
1063 | |                     {
1064 | |                         "level": alert.level.value,
1065 | |                         "component": alert.component,
1066 | |                         "message": alert.message,
1067 | |                         "metric_name": alert.metric_name,
1068 | |                         "metric_value": alert.metric_value,
1069 | |                         "threshold": alert.threshold,
1070 | |                         "timestamp": alert.timestamp
1071 | |                     } for alert in self.alerts
1072 | |                 ],
1073 | |                 "error_timestamps": list(self.error_timestamps),
1074 | |                 "error_counts": dict(self.error_counts),
1075 | |                 "api_response_times": list(self.api_response_times),
1076 | |                 "page_processing_times": list(self.page_processing_times),
1077 | |                 "memory_usage_history": list(self.memory_usage_history),
1078 | |                 "intervention_state": {
1079 | |                     "emergency_halt_requested": self._emergency_halt_requested,
1080 | |                     "immediate_intervention_requested": self._immediate_intervention_requested,
1081 | |                     "enhanced_monitoring_active": self._enhanced_monitoring_active,
1082 | |                     "monitoring_interval": self._monitoring_interval,
1083 | |                     "last_cleanup_time": self._last_cleanup_time
1084 | |                 },
1085 | |                 "performance_stats": self.get_performance_stats()
1086 | |             }
1087 | |
1088 | |             # Save checkpoint to file
1089 | |             checkpoint_file = checkpoint_dir / f"{checkpoint_name}.json"
1090 | |             with checkpoint_file.open('w', encoding='utf-8') as f:
1091 | |                 json.dump(session_state, f, indent=2, default=str)
1092 | |
1093 | |             logger.info(f"üìÅ Session checkpoint created: {checkpoint_name}")
1094 | |             return str(checkpoint_file)
1095 | |
1096 | |         except Exception as e:
1097 | |             logger.error(f"Failed to create session checkpoint: {e}")
1098 | |             return ""
1099 | |
1100 | |     def _restore_metrics_from_state(self, session_state: dict[str, Any]) -> None:
1101 | |         """Restore current metrics from session state."""
1102 | |         if "current_metrics" not in session_state:
1103 | |             return
1104 | |
1105 | |         for name, metric_data in session_state["current_metrics"].items():
1106 | |             metric = HealthMetric(
1107 | |                 name=name,
1108 | |                 value=metric_data["value"],
1109 | |                 threshold_warning=metric_data["threshold_warning"],
1110 | |                 threshold_critical=metric_data["threshold_critical"],
1111 | |                 timestamp=metric_data.get("timestamp", time.time())
1112 | |             )
1113 | |             self.current_metrics[name] = metric
1114 | |
1115 | |     def _restore_alerts_from_state(self, session_state: dict[str, Any]) -> None:
1116 | |         """Restore alerts from session state."""
1117 | |         if "alerts" not in session_state:
1118 | |             return
1119 | |
1120 | |         self.alerts = []
1121 | |         for alert_data in session_state["alerts"]:
1122 | |             level = AlertLevel(alert_data["level"])
1123 | |             alert = HealthAlert(
1124 | |                 level=level,
1125 | |                 component=alert_data["component"],
1126 | |                 message=alert_data["message"],
1127 | |                 metric_name=alert_data["metric_name"],
1128 | |                 metric_value=alert_data["metric_value"],
1129 | |                 threshold=alert_data["threshold"],
1130 | |                 timestamp=alert_data["timestamp"]
1131 | |             )
1132 | |             self.alerts.append(alert)
1133 | |
1134 | |     def _restore_error_data_from_state(self, session_state: dict[str, Any]) -> None:
1135 | |         """Restore error tracking data from session state."""
1136 | |         if "error_timestamps" in session_state:
1137 | |             self.error_timestamps = deque(session_state["error_timestamps"], maxlen=self.error_timestamps.maxlen)
1138 | |
1139 | |         if "error_counts" in session_state:
1140 | |             self.error_counts.update(session_state["error_counts"])
1141 | |
1142 | |     def _restore_performance_data_from_state(self, session_state: dict[str, Any]) -> None:
1143 | |         """Restore performance tracking data from session state."""
1144 | |         if "api_response_times" in session_state:
1145 | |             self.api_response_times = deque(session_state["api_response_times"], maxlen=self.api_response_times.maxlen)
1146 | |
1147 | |         if "page_processing_times" in session_state:
1148 | |             self.page_processing_times = deque(session_state["page_processing_times"], maxlen=self.page_processing_times.maxlen)
1149 | |
1150 | |         if "memory_usage_history" in session_state:
1151 | |             self.memory_usage_history = deque(session_state["memory_usage_history"], maxlen=self.memory_usage_history.maxlen)
1152 | |
1153 | |     def _restore_intervention_state_from_state(self, session_state: dict[str, Any]) -> None:
1154 | |         """Restore intervention state from session state."""
1155 | |         if "intervention_state" not in session_state:
1156 | |             return
1157 | |
1158 | |         intervention = session_state["intervention_state"]
1159 | |         self._emergency_halt_requested = intervention.get("emergency_halt_requested", False)
1160 | |         self._immediate_intervention_requested = intervention.get("immediate_intervention_requested", False)
1161 | |         self._enhanced_monitoring_active = intervention.get("enhanced_monitoring_active", False)
1162 | |         self._monitoring_interval = intervention.get("monitoring_interval", 30.0)
1163 | |         self._last_cleanup_time = intervention.get("last_cleanup_time", time.time())
1164 | |
1165 | |     def restore_from_checkpoint(self, checkpoint_path: str) -> bool:
1166 | |         """Restore session state from a checkpoint file."""
1167 | |         try:
1168 | |             checkpoint_file = Path(checkpoint_path)
1169 | |             if not checkpoint_file.exists():
1170 | |                 logger.error(f"Checkpoint file not found: {checkpoint_path}")
1171 | |                 return False
1172 | |
1173 | |             # Load checkpoint data
1174 | |             with checkpoint_file.open(encoding='utf-8') as f:
1175 | |                 session_state = json.load(f)
1176 | |
1177 | |             # Restore session state
1178 | |             self.session_start_time = session_state.get("session_start_time", time.time())
1179 | |
1180 | |             # Restore health score history
1181 | |             if "health_score_history" in session_state:
1182 | |                 self.health_score_history = deque(session_state["health_score_history"], maxlen=100)
1183 | |
1184 | |             # Restore all state components
1185 | |             self._restore_metrics_from_state(session_state)
1186 | |             self._restore_alerts_from_state(session_state)
1187 | |             self._restore_error_data_from_state(session_state)
1188 | |             self._restore_performance_data_from_state(session_state)
1189 | |             self._restore_intervention_state_from_state(session_state)
1190 | |
1191 | |             logger.info(f"üîÑ Session state restored from checkpoint: {checkpoint_file.name}")
1192 | |             return True
1193 | |
1194 | |         except Exception as e:
1195 | |             logger.error(f"Failed to restore from checkpoint: {e}")
1196 | |             return False
1197 | |
1198 | |     def auto_checkpoint(self, interval_minutes: int = 30) -> None:
1199 | |         """Automatically create checkpoints at regular intervals."""
1200 | |         try:
1201 | |             current_time = time.time()
1202 | |             if not hasattr(self, '_last_checkpoint_time'):
1203 | |                 self._last_checkpoint_time = current_time
1204 | |
1205 | |             # Check if it's time for a checkpoint
1206 | |             if current_time - self._last_checkpoint_time >= (interval_minutes * 60):
1207 | |                 checkpoint_name = f"auto_checkpoint_{int(current_time)}"
1208 | |                 self.create_session_checkpoint(checkpoint_name)
1209 | |                 self._last_checkpoint_time = current_time
1210 | |
1211 | |                 # Clean up old auto checkpoints (keep only last 5)
1212 | |                 self._cleanup_old_checkpoints()
1213 | |
1214 | |         except Exception as e:
1215 | |             logger.debug(f"Auto checkpoint failed: {e}")
1216 | |
1217 | |     @staticmethod
1218 | |     def _cleanup_old_checkpoints(keep_count: int = 5) -> None:
1219 | |         """Clean up old checkpoint files, keeping only the most recent ones."""
1220 | |         try:
1221 | |             checkpoint_dir = Path("Cache/session_checkpoints")
1222 | |             if not checkpoint_dir.exists():
1223 | |                 return
1224 | |
1225 | |             # Get all auto checkpoint files
1226 | |             auto_checkpoints = list(checkpoint_dir.glob("auto_checkpoint_*.json"))
1227 | |
1228 | |             # Sort by modification time (newest first)
1229 | |             auto_checkpoints.sort(key=lambda x: x.stat().st_mtime, reverse=True)
1230 | |
1231 | |             # Remove old checkpoints
1232 | |             for checkpoint in auto_checkpoints[keep_count:]:
1233 | |                 try:
1234 | |                     checkpoint.unlink()
1235 | |                     logger.debug(f"Removed old checkpoint: {checkpoint.name}")
1236 | |                 except Exception as e:
1237 | |                     logger.debug(f"Failed to remove checkpoint {checkpoint.name}: {e}")
1238 | |
1239 | |         except Exception as e:
1240 | |             logger.debug(f"Checkpoint cleanup failed: {e}")
1241 | |         else:
1242 | |             try:
1243 | |                 from cache_retention import auto_enforce_retention
1244 | |
1245 | |                 auto_enforce_retention("session_checkpoints")
1246 | |             except Exception as retention_error:
1247 | |                 logger.debug("Retention sweep for session checkpoints skipped: %s", retention_error)
1248 | |
1249 | |     @staticmethod
1250 | |     def list_available_checkpoints() -> list[dict[str, Any]]:
1251 | |         """List all available checkpoint files with metadata."""
1252 | |         try:
1253 | |             checkpoint_dir = Path("Cache/session_checkpoints")
1254 | |             if not checkpoint_dir.exists():
1255 | |                 return []
1256 | |
1257 | |             checkpoints: list[dict[str, Any]] = []
1258 | |             for checkpoint_file in checkpoint_dir.glob("*.json"):
1259 | |                 try:
1260 | |                     # Get file metadata
1261 | |                     stat = checkpoint_file.stat()
1262 | |
1263 | |                     # Try to read checkpoint metadata
1264 | |                     with checkpoint_file.open(encoding='utf-8') as f:
1265 | |                         data = json.load(f)
1266 | |
1267 | |                     checkpoints.append({
1268 | |                         "name": checkpoint_file.stem,
1269 | |                         "file_path": str(checkpoint_file),
1270 | |                         "created_time": data.get("timestamp", stat.st_mtime),
1271 | |                         "file_size_kb": stat.st_size / 1024,
1272 | |                         "session_start_time": data.get("session_start_time"),
1273 | |                         "health_score": data.get("performance_stats", {}).get("health_score", "N/A"),
1274 | |                         "total_errors": data.get("performance_stats", {}).get("total_errors", 0)
1275 | |                     })
1276 | |
1277 | |                 except Exception as e:
1278 | |                     logger.debug(f"Failed to read checkpoint metadata for {checkpoint_file.name}: {e}")
1279 | |
1280 | |             # Sort by creation time (newest first)
1281 | |             checkpoints.sort(key=lambda x: x["created_time"], reverse=True)
1282 | |             return checkpoints
1283 | |
1284 | |         except Exception as e:
1285 | |             logger.error(f"Failed to list checkpoints: {e}")
1286 | |             return []
1287 | |
1288 | |     def persist_session_state_to_disk(self, session_data: Optional[dict[str, Any]] = None) -> str:
1289 | |         """Persist current session state to disk for crash recovery."""
1290 | |         try:
1291 | |             # Create persistent state directory
1292 | |             state_dir = Path("Cache/session_state")
1293 | |             state_dir.mkdir(parents=True, exist_ok=True)
1294 | |
1295 | |             # Prepare session state data
1296 | |             if session_data is None:
1297 | |                 session_data = {}
1298 | |
1299 | |             # Add health monitoring state
1300 | |             session_data.update({
1301 | |                 "health_monitor": {
1302 | |                     "session_start_time": self.session_start_time,
1303 | |                     "current_metrics": {name: metric.value for name, metric in self.current_metrics.items()},
1304 | |                     "error_count": len(self.error_timestamps),
1305 | |                     "alert_count": len(self.alerts),
1306 | |                     "health_score": self.calculate_health_score(),
1307 | |                     "intervention_state": {
1308 | |                         "emergency_halt": self._emergency_halt_requested,
1309 | |                         "immediate_intervention": self._immediate_intervention_requested,
1310 | |                         "enhanced_monitoring": self._enhanced_monitoring_active
1311 | |                     }
1312 | |                 },
1313 | |                 "timestamp": time.time()
1314 | |             })
1315 | |
1316 | |             # Save to persistent state file
1317 | |             state_file = state_dir / "current_session.json"
1318 | |             with state_file.open('w', encoding='utf-8') as f:
1319 | |                 json.dump(session_data, f, indent=2, default=str)
1320 | |
1321 | |             logger.debug(f"Session state persisted to disk: {state_file}")
1322 | |
1323 | |             try:
1324 | |                 from cache_retention import auto_enforce_retention
1325 | |
1326 | |                 auto_enforce_retention("session_state")
1327 | |             except Exception as retention_error:
1328 | |                 logger.debug("Retention sweep for session state skipped: %s", retention_error)
1329 | |
1330 | |             return str(state_file)
1331 | |
1332 | |         except Exception as e:
1333 | |             logger.error(f"Failed to persist session state: {e}")
1334 | |             return ""
1335 | |
1336 | |     def recover_session_state_from_disk(self) -> Optional[dict[str, Any]]:
1337 | |         """Recover session state from disk after a crash."""
1338 | |         try:
1339 | |             state_file = Path("Cache/session_state/current_session.json")
1340 | |             if not state_file.exists():
1341 | |                 logger.info("No previous session state found")
1342 | |                 return None
1343 | |
1344 | |             # Load session state
1345 | |             with state_file.open(encoding='utf-8') as f:
1346 | |                 session_data = json.load(f)
1347 | |
1348 | |             # Check if state is recent (within last 24 hours)
1349 | |             state_age = time.time() - session_data.get("timestamp", 0)
1350 | |             if state_age > 86400:  # 24 hours
1351 | |                 logger.info("Previous session state is too old, ignoring")
1352 | |                 return None
1353 | |
1354 | |             # Restore health monitoring state if available
1355 | |             if "health_monitor" in session_data:
1356 | |                 health_data = session_data["health_monitor"]
1357 | |
1358 | |                 # Restore basic state
1359 | |                 if "session_start_time" in health_data:
1360 | |                     self.session_start_time = health_data["session_start_time"]
1361 | |
1362 | |                 # Restore intervention state
1363 | |                 if "intervention_state" in health_data:
1364 | |                     intervention = health_data["intervention_state"]
1365 | |                     self._emergency_halt_requested = intervention.get("emergency_halt", False)
1366 | |                     self._immediate_intervention_requested = intervention.get("immediate_intervention", False)
1367 | |                     self._enhanced_monitoring_active = intervention.get("enhanced_monitoring", False)
1368 | |
1369 | |             logger.info(f"üîÑ Session state recovered from disk (age: {state_age / 60:.1f} minutes)")
1370 | |             return session_data
1371 | |
1372 | |         except Exception as e:
1373 | |             logger.error(f"Failed to recover session state: {e}")
1374 | |             return None
     | |_______________________^
     |

Found 1 error.
